---
title: "Initial analysis"
author: "Jo√£o Vitor"
date: "06/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Paper data acquisition

Loading necessary packages

```{r}
library(tidyverse)
library(rvest)
library(httr)
```

This functions acquires papers from a selected country

```{r get-country-papers}
get_country_papers <- function(country) {
  url <- paste0('https://www.ncbi.nlm.nih.gov/research/coronavirus-api/export/tsv?filters=%7B%22countries%22%3A%5B%22', 
                country,
                '%22%5D%7D')
  
  read_tsv(url, skip = 31) %>% 
    mutate(paper_link = paste0("https://pubmed.ncbi.nlm.nih.gov/", pmid))
}
```

This one gets the paper's abstracts from pubmed, use this with care, since
pubmed has, justifiably, strict rate limiting. I've commented out a get-around using sys.sleep
that is necessary for larger queries.

```{r get_paper_abstract}
get_paper_abstract <- function(link) {
  
  message(paste("Current abstract:",link))
  
  read_html(link) %>% 
    html_nodes(css = "#en-abstract > p:nth-child(1)") %>% 
    html_text() %>% 
    str_trim()
  
  #Sys.sleep(20)
}
```

```{r}
get_doi_from_pmid <- function(pmid) {

  pmid <- as.character(pmid)
  
  url <- paste0("https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=",
                pmid,
                "&retmode=json")
  
  jsonlite::fromJSON(url)$result %>% 
    '[['(pmid) %>% 
    '[['('articleids') %>% 
    filter(idtype == 'doi') %>% 
    pull(value)
}
```

Let's take a look at Brazil first.

```{r}
br <- get_country_papers('China') %>% 
  mutate(abstract = map(paper_link, possibly(get_paper_abstract, otherwise = NA)),
         doi = map_chr(pmid, possibly(get_doi_from_pmid, otherwise = NA)))
```

## Named entity recognition workflow

```{r}
library(spacyr)

spacy_initialize(model = "en_core_sci_lg", condaenv = "~/miniconda3/envs/spacy-proc")
```

```{r}

pubtator_annot <- jsonlite::fromJSON("./litcovid2BioCJSON")[[2]] %>% 
  as_tibble() %>% 
  head(2000)

```

```{r}
texts <- c()

for (t in pubtator_annot$passages) {
  catted <- paste(t[[3]], collapse = ' ')
  texts <- append(texts, catted)
}

pubtator_annot <- pubtator_annot %>% 
  select(-c(`_id`, authors, infons)) %>% 
  mutate(text = texts) %>%
  unnest(cols = c(tags, accessions))
```

```{r}

# https://github.com/dgrtwo/data-screencasts/blob/master/cord-19.Rmd

tokenize_scispacy_entities <- function(data, text) {
  spacy_extract_entity(text) %>%
    group_by(doc_id) %>%
    nest() %>%
    pull(data) %>%
    map("text") %>%
    map(str_to_lower)
}

pubt_entities <- pubtator_annot %>% 
  select(id, text) %>% 
  tokenize_scispacy_entities(text = pubtator_annot$text) %>% 
  tibble() %>% 
  rename_at(vars(starts_with(".")), funs(str_replace(., ".", "entities"))) %>% 
  mutate(pmid = pubtator_annot$pmid) %>% 
  unnest_longer(entities)



```

